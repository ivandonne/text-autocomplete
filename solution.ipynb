{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7730769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\Deep Learning\\Sprint2\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "from src.data_utils import clean_string\n",
    "from src.data_utils import save_results_to_file\n",
    "from src.lstm_model import LSTMLanguageModel\n",
    "from src.lstm_model import calculate_rouge_batch\n",
    "\n",
    "\n",
    "MAX_SEQUENCE_LEN = 80\n",
    "VOCAB_SIZE = 50257  # для GPT-2 токенизатора\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 5\n",
    "MODEL_NAME = 'distilgpt2'\n",
    "OUTPUT_FILE = 'results/all_experiments.json'\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130448d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### ЭТАП 1. Загрузка датасета, очистка, токенизация, разбиение\n",
    "# загрузка датасета\n",
    "raw = pd.read_csv('./data/tweets100.txt', sep='\\t', header=None, names=['tweets'],\n",
    "                  on_bad_lines='skip')\n",
    "\n",
    "tweets = raw['tweets']\n",
    "# \"чистим\" тексты\n",
    "cleaned_tweets = raw['tweets'].apply(clean_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92cc164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)# создайте токенизатор\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'  # Left-padding нужен для decoder-only моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb7b87ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размеры выборок:\n",
      "Train: 80 samples\n",
      "Validation: 10 samples\n",
      "Test: 10 samples\n"
     ]
    }
   ],
   "source": [
    "# Токенизация текстов\n",
    "def tokenize_texts(texts):\n",
    "    tokenized = tokenizer(\n",
    "        texts.tolist(),\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=MAX_SEQUENCE_LEN,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# Токенизируем все тексты\n",
    "tokenized_data = tokenize_texts(cleaned_tweets)\n",
    "input_ids = tokenized_data['input_ids']\n",
    "attention_mask = tokenized_data['attention_mask']\n",
    "\n",
    "# Разделение на train (80%), validation (10%), test (10%)\n",
    "X_temp, X_test, mask_temp, mask_test = train_test_split(\n",
    "    input_ids, attention_mask, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_val, mask_train, mask_val = train_test_split(\n",
    "    X_temp, mask_temp, test_size=0.111, random_state=42  # 0.111 = 10% / 90%\n",
    ")\n",
    "\n",
    "print(f\"Размеры выборок:\")\n",
    "print(f\"Train: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation: {X_val.shape[0]} samples\")\n",
    "print(f\"Test: {X_test.shape[0]} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e62a9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание DataLoader'ов\n",
    "train_dataset = TensorDataset(X_train, mask_train)\n",
    "val_dataset = TensorDataset(X_val, mask_val)\n",
    "test_dataset = TensorDataset(X_test, mask_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d70a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ЧАСТЬ 1: ОБУЧЕНИЕ LSTM МОДЕЛИ\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#####################################################\n",
    "#             Этап 3. Работа с LSTM моделью         #\n",
    "#####################################################\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ЧАСТЬ 1: ОБУЧЕНИЕ LSTM МОДЕЛИ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Инициализация модели\n",
    "lstm_model = LSTMLanguageModel(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78c5a899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начинаем обучение LSTM модели...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 1/1 [00:04<00:00,  4.03s/it, loss=10.8324]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 10.8324, ROUGE-1 = 0.0000, ROUGE-2 = 0.0000, ROUGE-L = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 1/1 [00:04<00:00,  4.14s/it, loss=10.8001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss = 10.8001, ROUGE-1 = 0.0000, ROUGE-2 = 0.0000, ROUGE-L = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 1/1 [00:04<00:00,  4.85s/it, loss=10.7614]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss = 10.7614, ROUGE-1 = 0.0000, ROUGE-2 = 0.0000, ROUGE-L = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 1/1 [00:04<00:00,  4.85s/it, loss=10.6972]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Loss = 10.6972, ROUGE-1 = 0.0000, ROUGE-2 = 0.0000, ROUGE-L = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 1/1 [00:04<00:00,  4.93s/it, loss=10.5783]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss = 10.5783, ROUGE-1 = 0.0000, ROUGE-2 = 0.0000, ROUGE-L = 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Обучение модели\n",
    "train_losses = []\n",
    "val_rouge_scores = []\n",
    "\n",
    "print(\"Начинаем обучение LSTM модели...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    lstm_model .train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "    \n",
    "    for batch_inputs, batch_masks in progress_bar:\n",
    "        batch_inputs = batch_inputs.to(DEVICE)\n",
    "        batch_masks = batch_masks.to(DEVICE)\n",
    "        \n",
    "        # Подготовка данных: X = все кроме последнего токена, y = все кроме первого\n",
    "        X = batch_inputs[:, :-1]\n",
    "        y = batch_inputs[:, 1:]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output, _ = lstm_model (X)\n",
    "        \n",
    "        # Reshape для loss function\n",
    "        loss = criterion(output.reshape(-1, VOCAB_SIZE), y.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    # Валидация и вычисление ROUGE\n",
    "    rouge_scores = calculate_rouge_batch(lstm_model , tokenizer, val_loader, DEVICE, BATCH_SIZE, \n",
    "                                        'lstm', num_samples=50)\n",
    "    val_rouge_scores.append(rouge_scores)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}: Loss = {avg_loss:.4f}, '\n",
    "          f'ROUGE-1 = {rouge_scores[\"rouge1\"]:.4f}, '\n",
    "          f'ROUGE-2 = {rouge_scores[\"rouge2\"]:.4f}, '\n",
    "          f'ROUGE-L = {rouge_scores[\"rougeL\"]:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3e46eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Тестирование на тестовой выборке...\n",
      "Test ROUGE-1: 0.0000\n",
      "Test ROUGE-2: 0.0000\n",
      "Test ROUGE-L: 0.0000\n",
      "\n",
      "Результаты добавлены в файл: results/all_experiments.json\n",
      "Всего экспериментов в файле: 1\n"
     ]
    }
   ],
   "source": [
    "# Тестирование на тестовой выборке\n",
    "print(\"\\nТестирование на тестовой выборке...\")\n",
    "lstm_test_rouge = calculate_rouge_batch(lstm_model, tokenizer, test_loader, DEVICE, BATCH_SIZE, \n",
    "                                    'lstm', num_samples=100)\n",
    "print(f\"Test ROUGE-1: {lstm_test_rouge['rouge1']:.4f}\")\n",
    "print(f\"Test ROUGE-2: {lstm_test_rouge['rouge2']:.4f}\")\n",
    "print(f\"Test ROUGE-L: {lstm_test_rouge['rougeL']:.4f}\")\n",
    "\n",
    "# Сохраняем результаты в файл\n",
    "save_results_to_file(lstm_test_rouge, \n",
    "                    OUTPUT_FILE, \n",
    "                    'LSTM_custom', \n",
    "                    MAX_SEQUENCE_LEN, \n",
    "                    'lstm', \n",
    "                    experiment_name='LSTM_256_hidden',\n",
    "                    additional_info={'hidden_dim': HIDDEN_DIM, 'num_layers': NUM_LAYERS})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bd7edd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Примеры автодополнений:\n",
      "\n",
      "Примеры автодополнений LSTM:\n",
      "\n",
      "LSTM Пример 1:\n",
      "Промпт: ''\n",
      "Эталон: ''\n",
      "Сгенерировано: ' offenses retailerbehind PIT agent Cry ATMverse Mountains foo.[ twe shower slate 426volt Albuquerque revised̶ometown'\n",
      "\n",
      "LSTM Пример 2:\n",
      "Промпт: 'put vacation photos online'\n",
      "Эталон: ' a few yrs ago'\n",
      "Сгенерировано: ' MRI indie Skin ferry Writinst Bravo processedCanadianenthal towering�patientittered rookieroller Quentin trauma −multi'\n",
      "\n",
      "LSTM Пример 3:\n",
      "Промпт: 'oh haha dude i dont really look at em unless someone says hey i added you'\n",
      "Эталон: ' sorry im so terrible at that i'\n",
      "Сгенерировано: 'friendsPlayers sinceurred diplomats Puginois offsets almond Duke effect universally Schwartzgreraxinto Iro writingriver Doctors'\n"
     ]
    }
   ],
   "source": [
    "# Примеры генерации\n",
    "print(\"\\nПримеры автодополнений:\")\n",
    "# Примеры генерации LSTM\n",
    "print(\"\\nПримеры автодополнений LSTM:\")\n",
    "for i in range(3):\n",
    "    sample_input = X_test[i]\n",
    "    real_length = mask_test[i].sum().item()\n",
    "    prompt_length = int(real_length * 0.75)\n",
    "    \n",
    "    prompt_ids = sample_input[:prompt_length]\n",
    "    reference_ids = sample_input[prompt_length:real_length]\n",
    "    \n",
    "    prompt_text = tokenizer.decode(prompt_ids, skip_special_tokens=True)\n",
    "    reference_text = tokenizer.decode(reference_ids, skip_special_tokens=True)\n",
    "    \n",
    "    generated = lstm_model.generate(prompt_ids.unsqueeze(0), max_new_tokens=20, temperature=0.7)\n",
    "    generated_tokens = generated[0, prompt_ids.shape[0]:]\n",
    "    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\nLSTM Пример {i+1}:\")\n",
    "    print(f\"Промпт: '{prompt_text}'\")\n",
    "    print(f\"Эталон: '{reference_text}'\")\n",
    "    print(f\"Сгенерировано: '{generated_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd661d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ЧАСТЬ 2: ТЕСТИРОВАНИЕ TRANSFORMER МОДЕЛИ\n",
      "============================================================\n",
      "Тестирование Transformer на тестовой выборке...\n",
      "Transformer Test ROUGE-1: 0.0370\n",
      "Transformer Test ROUGE-2: 0.0000\n",
      "Transformer Test ROUGE-L: 0.0185\n",
      "\n",
      "Результаты добавлены в файл: results/all_experiments.json\n",
      "Всего экспериментов в файле: 2\n",
      "\n",
      "=== ДОПОЛНИТЕЛЬНАЯ СТАТИСТИКА ===\n",
      "Размер тренировочной выборки: 80 примеров\n",
      "Размер валидационной выборки: 10 примеров\n",
      "Общий размер датасета: 100 твитов\n",
      "\n",
      "Файлы с результатами:\n",
      "- Детальный JSON: results/all_experiments.json\n"
     ]
    }
   ],
   "source": [
    "#####################################################\n",
    "# Этап 4. Использование предобученного трансформера #\n",
    "#####################################################\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ЧАСТЬ 2: ТЕСТИРОВАНИЕ TRANSFORMER МОДЕЛИ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Загрузка модели\n",
    "transformer_model  = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "transformer_model.eval()  # Переводим модель в режим оценки\n",
    "\n",
    "# Вычисление метрик ROUGE на тестовой выборке\n",
    "print(\"Тестирование Transformer на тестовой выборке...\")\n",
    "transformer_test_rouge = calculate_rouge_batch(transformer_model, tokenizer, test_loader, DEVICE, \n",
    "                                                BATCH_SIZE, 'transformer', num_samples=100)\n",
    "\n",
    "print(f\"Transformer Test ROUGE-1: {transformer_test_rouge['rouge1']:.4f}\")\n",
    "print(f\"Transformer Test ROUGE-2: {transformer_test_rouge['rouge2']:.4f}\")\n",
    "print(f\"Transformer Test ROUGE-L: {transformer_test_rouge['rougeL']:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Сохраняем результаты в файл\n",
    "save_results_to_file( transformer_test_rouge,\n",
    "            OUTPUT_FILE,  # ТОТ ЖЕ ФАЙЛ!\n",
    "            'distilgpt2',\n",
    "            MAX_SEQUENCE_LEN,\n",
    "            model_type='transformer',\n",
    "            experiment_name='DistilGPT2_baseline'\n",
    "            )\n",
    "\n",
    "# Дополнительная статистика\n",
    "print(f\"\\n=== ДОПОЛНИТЕЛЬНАЯ СТАТИСТИКА ===\")\n",
    "print(f\"Размер тренировочной выборки: {X_train.shape[0]} примеров\")\n",
    "print(f\"Размер валидационной выборки: {X_val.shape[0]} примеров\")\n",
    "print(f\"Общий размер датасета: {len(cleaned_tweets)} твитов\")\n",
    "\n",
    "# Выводим путь к файлам результатов\n",
    "print(f\"\\nФайлы с результатами:\")\n",
    "print(f\"- Детальный JSON: {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5052a562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Примеры автодополнений Transformer:\n",
      "\n",
      "Transformer Пример 1:\n",
      "Промпт: ''\n",
      "Эталон: ''\n",
      "Сгенерировано: ' The Department of Justice is looking into whether the U.S. government intentionally and intentionally misled its law'\n",
      "\n",
      "Transformer Пример 2:\n",
      "Промпт: 'put vacation photos online'\n",
      "Эталон: ' a few yrs ago'\n",
      "Сгенерировано: ' at:\n",
      "\n",
      "\n",
      "What are the main reasons why people may not like this post?\n",
      "\n",
      "'\n",
      "\n",
      "Transformer Пример 3:\n",
      "Промпт: 'oh haha dude i dont really look at em unless someone says hey i added you'\n",
      "Эталон: ' sorry im so terrible at that i'\n",
      "Сгенерировано: ', but that's really not my answer. I think that's because i am a good guy.'\n"
     ]
    }
   ],
   "source": [
    "# Примеры генерации Transformer\n",
    "print(\"\\nПримеры автодополнений Transformer:\")\n",
    "for i in range(3):\n",
    "    sample_input = X_test[i]\n",
    "    real_length = mask_test[i].sum().item()\n",
    "    prompt_length = int(real_length * 0.75)\n",
    "    \n",
    "    prompt_ids = sample_input[:prompt_length]\n",
    "    reference_ids = sample_input[prompt_length:real_length]\n",
    "    \n",
    "    prompt_text = tokenizer.decode(prompt_ids, skip_special_tokens=True)\n",
    "    reference_text = tokenizer.decode(reference_ids, skip_special_tokens=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated = transformer_model.generate(\n",
    "            prompt_ids.unsqueeze(0).to(DEVICE),\n",
    "            attention_mask=torch.ones_like(prompt_ids).unsqueeze(0).to(DEVICE),\n",
    "            max_new_tokens=20,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_tokens = generated[0, prompt_ids.shape[0]:]\n",
    "    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\nTransformer Пример {i+1}:\")\n",
    "    print(f\"Промпт: '{prompt_text}'\")\n",
    "    print(f\"Эталон: '{reference_text}'\")\n",
    "    print(f\"Сгенерировано: '{generated_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb8ef13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "СРАВНЕНИЕ РЕЗУЛЬТАТОВ\n",
      "============================================================\n",
      "LSTM ROUGE-1:     0.0000\n",
      "Transformer ROUGE-1: 0.0370\n",
      "\n",
      "LSTM ROUGE-2:     0.0000\n",
      "Transformer ROUGE-2: 0.0000\n",
      "\n",
      "LSTM ROUGE-L:     0.0000\n",
      "Transformer ROUGE-L: 0.0185\n"
     ]
    }
   ],
   "source": [
    "# ========== СРАВНЕНИЕ РЕЗУЛЬТАТОВ ==========\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"СРАВНЕНИЕ РЕЗУЛЬТАТОВ\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"LSTM ROUGE-1:     {lstm_test_rouge['rouge1']:.4f}\")\n",
    "print(f\"Transformer ROUGE-1: {transformer_test_rouge['rouge1']:.4f}\")\n",
    "print()\n",
    "print(f\"LSTM ROUGE-2:     {lstm_test_rouge['rouge2']:.4f}\")\n",
    "print(f\"Transformer ROUGE-2: {transformer_test_rouge['rouge2']:.4f}\")\n",
    "print()\n",
    "print(f\"LSTM ROUGE-L:     {lstm_test_rouge['rougeL']:.4f}\")\n",
    "print(f\"Transformer ROUGE-L: {transformer_test_rouge['rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc132564",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
