{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7730769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "from src.data_utils import clean_string\n",
    "from src.data_utils import save_results_to_file\n",
    "from src.data_utils import save_selection_to_file\n",
    "from src.lstm_model import LSTMLanguageModel\n",
    "from src.lstm_model import calculate_rouge_batch\n",
    "\n",
    "\n",
    "MAX_SEQUENCE_LEN = 80\n",
    "VOCAB_SIZE = 50257  # для GPT-2 токенизатора\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 5\n",
    "MODEL_NAME = 'distilgpt2'\n",
    "OUTPUT_FILE = 'results/all_experiments.json'\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130448d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### ЭТАП 1. Загрузка датасета, очистка, токенизация, разбиение\n",
    "# загрузка датасета\n",
    "#\n",
    "# Из-за проблем с запуском ВМ обучение прогоняется локально на CPU для небольшой выборки\n",
    "# для демонстрации работы кода\n",
    "#\n",
    "raw = pd.read_csv('./data/tweets_small.txt', sep='\\t', header=None, names=['tweets'],\n",
    "                  on_bad_lines='skip')\n",
    "\n",
    "tweets = raw['tweets']\n",
    "# \"чистим\" тексты\n",
    "cleaned_tweets = raw['tweets'].apply(clean_string)\n",
    "\n",
    "df = pd.DataFrame(cleaned_tweets, columns=['tweets'])\n",
    "df.to_csv('./data/cleaned_tweets_small.txt', index=False, header=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92cc164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)# создайте токенизатор\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'  # Left-padding нужен для decoder-only моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7b87ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Токенизация текстов\n",
    "def tokenize_texts(texts):\n",
    "    tokenized = tokenizer(\n",
    "        texts.tolist(),\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=MAX_SEQUENCE_LEN,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# Токенизируем все тексты\n",
    "tokenized_data = tokenize_texts(cleaned_tweets)\n",
    "input_ids = tokenized_data['input_ids']\n",
    "attention_mask = tokenized_data['attention_mask']\n",
    "\n",
    "# Разделение на train (80%), validation (10%), test (10%)\n",
    "X_temp, X_test, mask_temp, mask_test = train_test_split(\n",
    "    input_ids, attention_mask, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_val, mask_train, mask_val = train_test_split(\n",
    "    X_temp, mask_temp, test_size=0.111, random_state=42  # 0.111 = 10% / 90%\n",
    ")\n",
    "\n",
    "print(f\"Размеры выборок:\")\n",
    "print(f\"Train: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation: {X_val.shape[0]} samples\")\n",
    "print(f\"Test: {X_test.shape[0]} samples\")\n",
    "\n",
    "save_selection_to_file(X_train, mask_train, tokenizer, './data/train.csv')\n",
    "save_selection_to_file(X_val, mask_val, tokenizer, './data/val.csv')\n",
    "save_selection_to_file(X_test, mask_test, tokenizer, './data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e62a9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание DataLoader'ов\n",
    "train_dataset = TensorDataset(X_train, mask_train)\n",
    "val_dataset = TensorDataset(X_val, mask_val)\n",
    "test_dataset = TensorDataset(X_test, mask_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d70a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "#             Этап 3. Работа с LSTM моделью         #\n",
    "#####################################################\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ЧАСТЬ 1: ОБУЧЕНИЕ LSTM МОДЕЛИ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Инициализация модели\n",
    "lstm_model = LSTMLanguageModel(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c5a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение модели\n",
    "train_losses = []\n",
    "val_rouge_scores = []\n",
    "\n",
    "print(\"Начинаем обучение LSTM модели...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    lstm_model .train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "    \n",
    "    for batch_inputs, batch_masks in progress_bar:\n",
    "        batch_inputs = batch_inputs.to(DEVICE)\n",
    "        batch_masks = batch_masks.to(DEVICE)\n",
    "        \n",
    "        # Подготовка данных: X = все кроме последнего токена, y = все кроме первого\n",
    "        X = batch_inputs[:, :-1]\n",
    "        y = batch_inputs[:, 1:]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output, _ = lstm_model (X)\n",
    "        \n",
    "        # Reshape для loss function\n",
    "        loss = criterion(output.reshape(-1, VOCAB_SIZE), y.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    # Валидация и вычисление ROUGE\n",
    "    rouge_scores = calculate_rouge_batch(lstm_model , tokenizer, val_loader, DEVICE, BATCH_SIZE, \n",
    "                                        'lstm', num_samples=50)\n",
    "    val_rouge_scores.append(rouge_scores)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}: Loss = {avg_loss:.4f}, '\n",
    "          f'ROUGE-1 = {rouge_scores[\"rouge1\"]:.4f}, '\n",
    "          f'ROUGE-2 = {rouge_scores[\"rouge2\"]:.4f}, '\n",
    "          f'ROUGE-L = {rouge_scores[\"rougeL\"]:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e46eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тестирование на тестовой выборке\n",
    "print(\"\\nТестирование на тестовой выборке...\")\n",
    "lstm_test_rouge = calculate_rouge_batch(lstm_model, tokenizer, test_loader, DEVICE, BATCH_SIZE, \n",
    "                                    'lstm', num_samples=100)\n",
    "print(f\"Test ROUGE-1: {lstm_test_rouge['rouge1']:.4f}\")\n",
    "print(f\"Test ROUGE-2: {lstm_test_rouge['rouge2']:.4f}\")\n",
    "print(f\"Test ROUGE-L: {lstm_test_rouge['rougeL']:.4f}\")\n",
    "\n",
    "# Сохраняем результаты в файл\n",
    "save_results_to_file(lstm_test_rouge, \n",
    "                    OUTPUT_FILE, \n",
    "                    'LSTM_custom', \n",
    "                    MAX_SEQUENCE_LEN, \n",
    "                    'lstm', \n",
    "                    experiment_name='LSTM_256_hidden',\n",
    "                    additional_info={'hidden_dim': HIDDEN_DIM, 'num_layers': NUM_LAYERS})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd7edd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Примеры генерации\n",
    "print(\"\\nПримеры автодополнений:\")\n",
    "# Примеры генерации LSTM\n",
    "print(\"\\nПримеры автодополнений LSTM:\")\n",
    "for i in range(3):\n",
    "    sample_input = X_test[i]\n",
    "    real_length = mask_test[i].sum().item()\n",
    "    prompt_length = int(real_length * 0.75)\n",
    "    \n",
    "    prompt_ids = sample_input[:prompt_length]\n",
    "    reference_ids = sample_input[prompt_length:real_length]\n",
    "    \n",
    "    prompt_text = tokenizer.decode(prompt_ids, skip_special_tokens=True)\n",
    "    reference_text = tokenizer.decode(reference_ids, skip_special_tokens=True)\n",
    "    \n",
    "    generated = lstm_model.generate(prompt_ids.unsqueeze(0), max_new_tokens=20, temperature=0.7)\n",
    "    generated_tokens = generated[0, prompt_ids.shape[0]:]\n",
    "    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\nLSTM Пример {i+1}:\")\n",
    "    print(f\"Промпт: '{prompt_text}'\")\n",
    "    print(f\"Эталон: '{reference_text}'\")\n",
    "    print(f\"Сгенерировано: '{generated_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd661d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# Этап 4. Использование предобученного трансформера #\n",
    "#####################################################\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ЧАСТЬ 2: ТЕСТИРОВАНИЕ TRANSFORMER МОДЕЛИ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Загрузка модели\n",
    "transformer_model  = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "transformer_model.eval()  # Переводим модель в режим оценки\n",
    "\n",
    "# Вычисление метрик ROUGE на тестовой выборке\n",
    "print(\"Тестирование Transformer на тестовой выборке...\")\n",
    "transformer_test_rouge = calculate_rouge_batch(transformer_model, tokenizer, test_loader, DEVICE, \n",
    "                                                BATCH_SIZE, 'transformer', num_samples=100)\n",
    "\n",
    "print(f\"Transformer Test ROUGE-1: {transformer_test_rouge['rouge1']:.4f}\")\n",
    "print(f\"Transformer Test ROUGE-2: {transformer_test_rouge['rouge2']:.4f}\")\n",
    "print(f\"Transformer Test ROUGE-L: {transformer_test_rouge['rougeL']:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Сохраняем результаты в файл\n",
    "save_results_to_file( transformer_test_rouge,\n",
    "            OUTPUT_FILE,  # ТОТ ЖЕ ФАЙЛ!\n",
    "            'distilgpt2',\n",
    "            MAX_SEQUENCE_LEN,\n",
    "            model_type='transformer',\n",
    "            experiment_name='DistilGPT2_baseline'\n",
    "            )\n",
    "\n",
    "# Дополнительная статистика\n",
    "print(f\"\\n=== ДОПОЛНИТЕЛЬНАЯ СТАТИСТИКА ===\")\n",
    "print(f\"Размер тренировочной выборки: {X_train.shape[0]} примеров\")\n",
    "print(f\"Размер валидационной выборки: {X_val.shape[0]} примеров\")\n",
    "print(f\"Общий размер датасета: {len(cleaned_tweets)} твитов\")\n",
    "\n",
    "# Выводим путь к файлам результатов\n",
    "print(f\"\\nФайлы с результатами:\")\n",
    "print(f\"- Детальный JSON: {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5052a562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Примеры генерации Transformer\n",
    "print(\"\\nПримеры автодополнений Transformer:\")\n",
    "for i in range(3):\n",
    "    sample_input = X_test[i]\n",
    "    real_length = mask_test[i].sum().item()\n",
    "    prompt_length = int(real_length * 0.75)\n",
    "    \n",
    "    prompt_ids = sample_input[:prompt_length]\n",
    "    reference_ids = sample_input[prompt_length:real_length]\n",
    "    \n",
    "    prompt_text = tokenizer.decode(prompt_ids, skip_special_tokens=True)\n",
    "    reference_text = tokenizer.decode(reference_ids, skip_special_tokens=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated = transformer_model.generate(\n",
    "            prompt_ids.unsqueeze(0).to(DEVICE),\n",
    "            attention_mask=torch.ones_like(prompt_ids).unsqueeze(0).to(DEVICE),\n",
    "            max_new_tokens=20,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_tokens = generated[0, prompt_ids.shape[0]:]\n",
    "    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\nTransformer Пример {i+1}:\")\n",
    "    print(f\"Промпт: '{prompt_text}'\")\n",
    "    print(f\"Эталон: '{reference_text}'\")\n",
    "    print(f\"Сгенерировано: '{generated_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8ef13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== СРАВНЕНИЕ РЕЗУЛЬТАТОВ ==========\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"СРАВНЕНИЕ РЕЗУЛЬТАТОВ\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"LSTM ROUGE-1:     {lstm_test_rouge['rouge1']:.4f}\")\n",
    "print(f\"Transformer ROUGE-1: {transformer_test_rouge['rouge1']:.4f}\")\n",
    "print()\n",
    "print(f\"LSTM ROUGE-2:     {lstm_test_rouge['rouge2']:.4f}\")\n",
    "print(f\"Transformer ROUGE-2: {transformer_test_rouge['rouge2']:.4f}\")\n",
    "print()\n",
    "print(f\"LSTM ROUGE-L:     {lstm_test_rouge['rougeL']:.4f}\")\n",
    "print(f\"Transformer ROUGE-L: {transformer_test_rouge['rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc132564",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
